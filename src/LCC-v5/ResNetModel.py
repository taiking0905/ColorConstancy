import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import resnet18

from config import DROPOUT, OUTPUT_DIM, DEVICE

# ResNet18ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸã‚«ãƒ©ãƒ¼æ¨å®šãƒ¢ãƒ‡ãƒ«ï¼ˆå…¥åŠ›: 1ch, å‡ºåŠ›: RGBãƒ™ã‚¯ãƒˆãƒ«ï¼‰
class ResNetModel(nn.Module):
    def __init__(self, output_dim = OUTPUT_DIM, dropout_rate = DROPOUT):
        super().__init__()
        model = resnet18(weights=None)

        # å…¥åŠ›ãƒãƒ£ãƒ³ãƒãƒ«ã‚’1chï¼ˆã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã‚„ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ç”»åƒï¼‰ã«å¤‰æ›´
        model.conv1 = nn.Conv2d(
            in_channels=1,       # å…¥åŠ›ã¯1ãƒãƒ£ãƒãƒ«
            out_channels=64,     # å‡ºåŠ›ã¯64ãƒãƒ£ãƒãƒ«ï¼ˆResNetæ¨™æº–ï¼‰
            kernel_size=7,
            stride=2,
            padding=3,
            bias=False
        )
        
        self.model = model


        # æ—¢å­˜ã®fcå±¤ã®å‰ã«Dropoutã‚’æŒŸã‚€æ§‹é€ ã«å¤‰æ›´
        in_features = self.model.fc.in_features
        self.model.fc = nn.Sequential(
            nn.Dropout(p=dropout_rate),          # Dropoutã‚’è¿½åŠ 
            nn.Linear(in_features, output_dim)   # æœ€çµ‚å‡ºåŠ›
        )

        
    def forward(self, x):
        return self.model(x)  # é †ä¼æ’­ï¼ˆå‡ºåŠ›ã¯ shape: [batch_size, 3]ï¼‰


# ğŸ”ºè§’åº¦ãƒ™ãƒ¼ã‚¹ã®æå¤±é–¢æ•°ï¼ˆè‰²ãƒ™ã‚¯ãƒˆãƒ«ã®æ–¹å‘ã‚’æ¯”è¼ƒï¼‰
def angular_loss(pred, target):
    """
    pred: ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ› (N, 3)
    target: æ­£è§£ã®RGBæ¯”ç‡ãƒ™ã‚¯ãƒˆãƒ« (N, 3)
    â†’ å‡ºåŠ›ãƒ™ã‚¯ãƒˆãƒ«ã¨æ­£è§£ãƒ™ã‚¯ãƒˆãƒ«ã®è§’åº¦ï¼ˆcosé¡ä¼¼åº¦ï¼‰ã§èª¤å·®ã‚’è¨ˆç®—
    """
    pred_norm = F.normalize(pred, dim=1)     # å‡ºåŠ›ã‚’L2æ­£è¦åŒ–ï¼ˆé•·ã•ã‚’1ã«ï¼‰
    target_norm = F.normalize(target, dim=1) # æ­£è§£ã‚‚L2æ­£è¦åŒ–

    cos_sim = (pred_norm * target_norm).sum(dim=1)  # å„ãƒ™ã‚¯ãƒˆãƒ«é–“ã®cosé¡ä¼¼åº¦
    cos_sim = torch.clamp(cos_sim, -1.0, 1.0)
    
    loss = 1 - cos_sim  # cosÎ¸ãŒé«˜ã„ï¼ˆæ–¹å‘ãŒä¸€è‡´ï¼‰ã»ã©æå¤±ãŒå°ã•ã„
    return loss.mean()  # ãƒãƒƒãƒå¹³å‡ã®æå¤±ã‚’è¿”ã™


# ğŸ” 1ã‚¨ãƒãƒƒã‚¯åˆ†ã®è¨“ç·´å‡¦ç†

def train_one_epoch(model, loader, optimizer, loss_fn, accumulation_steps=1):
    # iter_start = time.time()
    # print(f"â±ï¸ First iter(loader): {time.time() - iter_start:.3f} sec")

    model.train()
    total_loss = 0.0
    optimizer.zero_grad()

    for i, (X_batch, y_batch) in enumerate(loader):
        X_batch = X_batch.to(DEVICE)
        y_batch = y_batch.to(DEVICE)

        start_event = torch.cuda.Event(enable_timing=True)
        end_event = torch.cuda.Event(enable_timing=True)

        start_event.record()
        pred = model(X_batch)
        loss = loss_fn(pred, y_batch)
        loss = loss / accumulation_steps  # ğŸ”‘ å‹¾é…ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

        loss.backward()
        end_event.record()

        # âœ… accumulation_stepså›ã”ã¨ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(loader):
            optimizer.step()
            optimizer.zero_grad()

        total_loss += loss.item() * accumulation_steps

    average_loss = total_loss / len(loader)
    return average_loss




# ğŸ” è©•ä¾¡é–¢æ•°ï¼ˆæ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆç”¨ï¼‰
def evaluate(model, loader, loss_fn):
    model.eval()  
    total_loss = 0.0

    with torch.no_grad():  # å‹¾é…ã‚’è¨ˆç®—ã—ãªã„ï¼ˆæ¨è«–ã®ã¿ã§é«˜é€Ÿãƒ»çœãƒ¡ãƒ¢ãƒªï¼‰

        for X_batch, y_batch in loader:
            X_batch = X_batch.to(DEVICE)
            y_batch = y_batch.to(DEVICE)

            pred = model(X_batch)               # ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›
            loss = loss_fn(pred, y_batch)       # æå¤±ã‚’è¨ˆç®—
            total_loss += loss.item()

    average_loss = total_loss / len(loader)     # å…¨ä½“ã®å¹³å‡æå¤±
    return average_loss
